{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# yoctoGPT on Google Colab (T4, ~12GB VRAM)\n",
        "\n",
        "This notebook mounts Google Drive for persistent checkpoints, clones the\n",
        "yoctoGPT repository, prepares a token dataset from all `.txt` files in\n",
        "`data/`, trains a speed-focused model sized for a Colab T4 (~12GB),\n",
        "provides two sampling examples, and includes a resume cell. It also\n",
        "adapts context/batch size when the corpus is tiny to avoid random index\n",
        "errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Mount Google Drive and set project directory (persist checkpoints)\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "project_dir = \"/content/drive/MyDrive/yocto\"  # adjust to your setup\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "CKPT_DIR = Path(project_dir) / \"checkpoints/colab_fast\"\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "os.chdir(project_dir)\n",
        "print(\"Working dir:\", os.getcwd())\n",
        "print(\"Checkpoints dir:\", CKPT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Setup: install deps and clone/update the repo\n",
        "!nvidia-smi || true\n",
        "!python -V\n",
        "!pip -q install tokenizers tqdm\n",
        "\n",
        "import os, pathlib, subprocess, textwrap\n",
        "\n",
        "repo_root = pathlib.Path(\"yoctoGPT\")\n",
        "if repo_root.exists():\n",
        "    print(\"Repo exists, pulling latest...\")\n",
        "    subprocess.run([\"git\", \"pull\"], cwd=repo_root, check=False)\n",
        "else:\n",
        "    !git clone https://github.com/yhilpisch/yoctoGPT.git\n",
        "os.chdir(repo_root)\n",
        "\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    !pip -q install -r requirements.txt || true\n",
        "\n",
        "data_dir = pathlib.Path(\"data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "txts = list(data_dir.glob(\"*.txt\"))\n",
        "if not txts:\n",
        "    sample = textwrap.dedent('''\n",
        "    Philosophy is the study of general and fundamental questions,\n",
        "    such as those about existence, reason, knowledge, values, mind,\n",
        "    and language. It often poses questions rather than providing\n",
        "    answers, inviting us to think.\n",
        "    ''').strip()\n",
        "    (data_dir / \"philosophy.txt\").write_text(sample, encoding=\"utf-8\")\n",
        "    print(\"Created sample data/philosophy.txt\")\n",
        "else:\n",
        "    names = [p.name for p in txts][:5]\n",
        "    print(f\"Found {len(txts)} text files in data/: {names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Tokenization: prepare token-level dataset from all .txt files\n",
        "!python -m scripts.prepare_tokenizer       --all_txt_in_dir --text_dir data --out_dir data/token       --vocab_size 8000 --random_split --split_seed 1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Pick safe block_size/batch_size for this corpus\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "train_path = Path(\"data/token/train.bin\")\n",
        "val_path = Path(\"data/token/val.bin\")\n",
        "train_tokens = int(np.fromfile(train_path, dtype=np.int32).shape[0])\n",
        "val_tokens = int(np.fromfile(val_path, dtype=np.int32).shape[0])\n",
        "min_tokens = min(train_tokens, val_tokens)\n",
        "\n",
        "if min_tokens <= 4:\n",
        "    raise SystemExit(\n",
        "        \"Dataset too small. Add more text to data/ and rerun tokenization.\"\n",
        "    )\n",
        "\n",
        "block_candidates = [512, 384, 256, 192, 128, 96, 64, 48, 32, 24, 16]\n",
        "block_size = next(\n",
        "    (b for b in block_candidates if min_tokens > b + 2),\n",
        "    max(8, min_tokens - 2),\n",
        ")\n",
        "\n",
        "target_tokens = min(6000, max(512, min_tokens))\n",
        "batch_size = max(1, min(24, target_tokens // block_size))\n",
        "\n",
        "print(\n",
        "    f\"Train tokens: {train_tokens}, Val tokens: {val_tokens}, \"\n",
        "    f\"min_tokens: {min_tokens}\"\n",
        ")\n",
        "print(f\"Using block_size={block_size}, batch_size={batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title (Optional) Get an auto-recommended command for this GPU (T4 ~12GB)\n",
        "!python -m scripts.recommend_training       --mode token       --data_dir data/token       --tokenizer_path data/token/tokenizer.json       --ckpt_dir {CKPT_DIR}       --priority speed       --device cuda       --device_mem_gb 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "We use the speed-focused variant `gpt_fast` (Flash/SDPA attention) with\n",
        "a configuration chosen to fit comfortably on a Colab T4 (~12GB). If you\n",
        "hit OOM, lower `batch_size` or `block_size`; if you have headroom, you\n",
        "can nudge them upward. The auto-picked block/batch above avoids tiny-\n",
        "corpus indexing errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Train (gpt_fast) on Colab T4\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(project_dir) / \"checkpoints/colab_fast\"\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "!python -m yoctoGPT.train       --mode token       --data_dir data/token       --tokenizer_path data/token/tokenizer.json       --ckpt_dir {CKPT_DIR}       --model_type gpt_fast       --device cuda       --n_layer 6 --n_head 6 --n_embd 384       --block_size {block_size} --batch_size {batch_size}       --dropout 0.15 --weight_decay 0.08       --tie_weights --label_smoothing 0.05       --eval_interval 400 --eval_iters 200       --cosine_lr --warmup_iters 400       --min_lr 1e-5 --lr 2e-4       --max_iters 1500       --ema --ema_decay 0.999"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling examples\n",
        "\n",
        "Generate text from the latest checkpoint. Adjust temperature/top-k/top-p\n",
        "for style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    #@title Sample 1: Q/A style prompt\n",
        "    from pathlib import Path\n",
        "\n",
        "    CKPT_DIR = Path(project_dir) / \"checkpoints/colab_fast\"\n",
        "\n",
        "    !python -m yoctoGPT.sampler       --mode token       --ckpt {CKPT_DIR}/latest.pt       --tokenizer_path data/token/tokenizer.json       --prompt \"Q: What is the meaning of life?\n",
        "A:\"       --max_new_tokens 200       --temperature 0.8 --top_k 50 --top_p 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Sample 2: Continuation\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(project_dir) / \"checkpoints/colab_fast\"\n",
        "\n",
        "!python -m yoctoGPT.sampler       --mode token       --ckpt {CKPT_DIR}/latest.pt       --tokenizer_path data/token/tokenizer.json       --prompt \"In the beginning, philosophy sought to\"       --max_new_tokens 200       --temperature 0.9 --top_k 40 --top_p 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resume training\n",
        "\n",
        "Resume from the latest checkpoint to continue training for additional\n",
        "steps. `--max_iters` is interpreted as extra steps beyond the\n",
        "checkpointed iteration count. Checkpoints are stored under\n",
        "`/content/drive/.../checkpoints/colab_fast`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Resume training from latest.pt (additional 800 steps)\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(project_dir) / \"checkpoints/colab_fast\"\n",
        "latest = CKPT_DIR / \"latest.pt\"\n",
        "if not latest.exists():\n",
        "    raise SystemExit(\"No latest.pt found in CKPT_DIR; run training first.\")\n",
        "\n",
        "!python -m yoctoGPT.train       --mode token       --data_dir data/token       --tokenizer_path data/token/tokenizer.json       --ckpt_dir {CKPT_DIR}       --resume {latest}       --model_type gpt_fast       --device cuda       --n_layer 6 --n_head 6 --n_embd 384       --block_size {block_size} --batch_size {batch_size}       --dropout 0.15 --weight_decay 0.08       --tie_weights --label_smoothing 0.05       --eval_interval 400 --eval_iters 200       --cosine_lr --warmup_iters 400       --min_lr 1e-5 --lr 2e-4       --max_iters 800       --ema --ema_decay 0.999"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optionally, inspect the last lines of the training metrics CSV to\n",
        "monitor progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Inspect metrics\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(project_dir) / \"checkpoints/colab_fast\"\n",
        "\n",
        "!tail -n 20 {CKPT_DIR}/metrics.csv || true"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}