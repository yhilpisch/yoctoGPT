{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# yoctoGPT on Google Colab (T4, ~12GB VRAM)\n",
        "\n",
        "This notebook mounts Google Drive for persistent checkpoints, keeps the\n",
        "repository local on Colab (not on Drive), prepares a token dataset from all\n",
        "`.txt` files in `data/`, trains a speed-focused model sized for a Colab T4\n",
        "(~12GB), provides two sampling examples, and includes a resume cell. It also\n",
        "adapts context/batch size when the corpus is tiny to avoid random index\n",
        "errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Mount Google Drive for checkpoint storage (repo stays local)\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_fast\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Checkpoints dir:\", CKPT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Setup: install deps and clone/update the repo locally\n",
        "!nvidia-smi || true\n",
        "!python -V\n",
        "!pip -q install tokenizers tqdm\n",
        "\n",
        "import os, pathlib, subprocess, textwrap\n",
        "\n",
        "repo_root = pathlib.Path(\"/content/yoctoGPT\")\n",
        "if repo_root.exists():\n",
        "    print(\"Repo exists, pulling latest...\")\n",
        "    subprocess.run([\"git\", \"pull\"], cwd=repo_root, check=False)\n",
        "else:\n",
        "    subprocess.run(\n",
        "        [\n",
        "            \"git\",\n",
        "            \"clone\",\n",
        "            \"https://github.com/yhilpisch/yoctoGPT.git\",\n",
        "            str(repo_root),\n",
        "        ],\n",
        "        check=False,\n",
        "    )\n",
        "os.chdir(repo_root)\n",
        "\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    !pip -q install -r requirements.txt || true\n",
        "\n",
        "data_dir = pathlib.Path(\"data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "txts = list(data_dir.glob(\"*.txt\"))\n",
        "if not txts:\n",
        "    sample = textwrap.dedent('''\n",
        "    Philosophy is the study of general and fundamental questions,\n",
        "    such as those about existence, reason, knowledge, values, mind,\n",
        "    and language. It often poses questions rather than providing\n",
        "    answers, inviting us to think.\n",
        "    ''').strip()\n",
        "    (data_dir / \"philosophy.txt\").write_text(sample, encoding=\"utf-8\")\n",
        "    print(\"Created sample data/philosophy.txt\")\n",
        "else:\n",
        "    names = [p.name for p in txts][:5]\n",
        "    print(f\"Found {len(txts)} text files in data/: {names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Tokenization: prepare token-level dataset from all .txt files\n",
        "!python -m scripts.prepare_tokenizer \\\n",
        "  --all_txt_in_dir \\\n",
        "  --text_dir data \\\n",
        "  --out_dir data/token \\\n",
        "  --vocab_size 8000 \\\n",
        "  --random_split \\\n",
        "  --split_seed 1337"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Pick safe block_size/batch_size for this corpus\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "train_path = Path(\"data/token/train.bin\")\n",
        "val_path = Path(\"data/token/val.bin\")\n",
        "train_tokens = int(np.fromfile(train_path, dtype=np.int32).shape[0])\n",
        "val_tokens = int(np.fromfile(val_path, dtype=np.int32).shape[0])\n",
        "min_tokens = min(train_tokens, val_tokens)\n",
        "\n",
        "if min_tokens <= 4:\n",
        "    raise SystemExit(\n",
        "        \"Dataset too small. Add more text to data/ and rerun tokenization.\"\n",
        "    )\n",
        "\n",
        "block_candidates = [512, 384, 256, 192, 128, 96, 64, 48, 32, 24, 16]\n",
        "block_size = next(\n",
        "    (b for b in block_candidates if min_tokens > b + 2),\n",
        "    max(8, min_tokens - 2),\n",
        ")\n",
        "\n",
        "target_tokens = min(6000, max(512, min_tokens))\n",
        "batch_size = max(1, min(24, target_tokens // block_size))\n",
        "\n",
        "print(\n",
        "    f\"Train tokens: {train_tokens}, Val tokens: {val_tokens}, \"\n",
        "    f\"min_tokens: {min_tokens}\"\n",
        ")\n",
        "print(f\"Using block_size={block_size}, batch_size={batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title (Optional) Get an auto-recommended command for this GPU (T4 ~12GB)\n",
        "!python -m scripts.recommend_training \\\n",
        "  --mode token \\\n",
        "  --data_dir data/token \\\n",
        "  --tokenizer_path data/token/tokenizer.json \\\n",
        "  --ckpt_dir {CKPT_DIR} \\\n",
        "  --priority speed \\\n",
        "  --device cuda \\\n",
        "  --device_mem_gb 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "We use the speed-focused variant `gpt_fast` (Flash/SDPA attention) with\n",
        "a configuration chosen to fit comfortably on a Colab T4 (~12GB). If you\n",
        "hit OOM, lower `batch_size` or `block_size`; if you have headroom, you\n",
        "can nudge them upward. The auto-picked block/batch above avoids tiny-\n",
        "corpus indexing errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Train (gpt_fast) on Colab T4\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_fast\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "!python -m yoctoGPT.train \\\n",
        "  --mode token \\\n",
        "  --data_dir data/token \\\n",
        "  --tokenizer_path data/token/tokenizer.json \\\n",
        "  --ckpt_dir {CKPT_DIR} \\\n",
        "  --model_type gpt_fast \\\n",
        "  --device cuda \\\n",
        "  --n_layer 6 --n_head 6 --n_embd 384 \\\n",
        "  --block_size {block_size} --batch_size {batch_size} \\\n",
        "  --dropout 0.15 --weight_decay 0.08 \\\n",
        "  --tie_weights --label_smoothing 0.05 \\\n",
        "  --eval_interval 400 --eval_iters 200 \\\n",
        "  --cosine_lr --warmup_iters 400 \\\n",
        "  --min_lr 1e-5 --lr 2e-4 \\\n",
        "  --max_iters 1500 \\\n",
        "  --ema --ema_decay 0.999"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling examples\n",
        "\n",
        "Generate text from the latest checkpoint. Adjust temperature/top-k/top-p\n",
        "for style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        #@title Sample 1: Q/A style prompt\n",
        "        from pathlib import Path\n",
        "\n",
        "        CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_fast\")\n",
        "\n",
        "        !python -m yoctoGPT.sampler \\\n",
        "          --mode token \\\n",
        "          --ckpt {CKPT_DIR}/latest.pt \\\n",
        "          --tokenizer_path data/token/tokenizer.json \\\n",
        "          --prompt \"Q: What is the meaning of life?\n",
        "A:\" \\\n",
        "          --max_new_tokens 200 \\\n",
        "          --temperature 0.8 --top_k 50 --top_p 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Sample 2: Continuation\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_fast\")\n",
        "\n",
        "!python -m yoctoGPT.sampler \\\n",
        "  --mode token \\\n",
        "  --ckpt {CKPT_DIR}/latest.pt \\\n",
        "  --tokenizer_path data/token/tokenizer.json \\\n",
        "  --prompt \"In the beginning, philosophy sought to\" \\\n",
        "  --max_new_tokens 200 \\\n",
        "  --temperature 0.9 --top_k 40 --top_p 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resume training\n",
        "\n",
        "Resume from the latest checkpoint to continue training for additional\n",
        "steps. `--max_iters` is interpreted as extra steps beyond the\n",
        "checkpointed iteration count. Checkpoints are stored under\n",
        "`/content/drive/MyDrive/yocto/checkpoints/colab_fast`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Resume training from latest.pt (additional 800 steps)\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_fast\")\n",
        "latest = CKPT_DIR / \"latest.pt\"\n",
        "if not latest.exists():\n",
        "    raise SystemExit(\n",
        "        \"No latest.pt found in CKPT_DIR; run training first.\"\n",
        "    )\n",
        "\n",
        "!python -m yoctoGPT.train \\\n",
        "  --mode token \\\n",
        "  --data_dir data/token \\\n",
        "  --tokenizer_path data/token/tokenizer.json \\\n",
        "  --ckpt_dir {CKPT_DIR} \\\n",
        "  --resume {latest} \\\n",
        "  --model_type gpt_fast \\\n",
        "  --device cuda \\\n",
        "  --n_layer 6 --n_head 6 --n_embd 384 \\\n",
        "  --block_size {block_size} --batch_size {batch_size} \\\n",
        "  --dropout 0.15 --weight_decay 0.08 \\\n",
        "  --tie_weights --label_smoothing 0.05 \\\n",
        "  --eval_interval 400 --eval_iters 200 \\\n",
        "  --cosine_lr --warmup_iters 400 \\\n",
        "  --min_lr 1e-5 --lr 2e-4 \\\n",
        "  --max_iters 800 \\\n",
        "  --ema --ema_decay 0.999"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optionally, inspect the last lines of the training metrics CSV to\n",
        "monitor progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Inspect metrics\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_fast\")\n",
        "\n",
        "!tail -n 20 {CKPT_DIR}/metrics.csv || true"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}