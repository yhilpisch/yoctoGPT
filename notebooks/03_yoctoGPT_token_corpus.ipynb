{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tae-logo"
      },
      "source": [
        "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
      ],
      "id": "tae-logo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tae-header"
      },
      "source": [
        "# yoctoGPT — Minimal GPT from Scratch\n",
        "\n",
        "## _Advanced Training — Token-level Training_\n",
        "\n",
        "**&copy; Dr. Yves J. Hilpisch**<br>AI-Powered by OpenAI & Gemini."
      ],
      "id": "tae-header"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "how-to-use"
      },
      "source": [
        "## How to Use This Notebook\n",
        "\n",
        "- **Goal**: Train a GPT model using a Byte Pair Encoding (BPE) tokenizer.\n",
        "- **Hardware**: Optimized for Google Colab T4 (~15GB VRAM).\n",
        "- **Persistence**: Mounts Google Drive for persistent checkpoint storage."
      ],
      "id": "how-to-use"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roadmap"
      },
      "source": [
        "### Roadmap\n",
        "\n",
        "1. **Setup**: Install dependencies, clone repo, and mount Google Drive.\n",
        "2. **Tokenization**: Train a BPE tokenizer and encode the corpus into subword tokens.\n",
        "3. **Training**: Train a model using the `gpt_fast` architecture for maximum throughput.\n",
        "4. **Sampling & Resume**: Generate text using the tokenizer and resume from checkpoints."
      ],
      "id": "roadmap"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive for checkpoint storage (repo stays local)\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_fast\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Checkpoints dir:\", CKPT_DIR)"
      ],
      "id": "mount-drive"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-code"
      },
      "outputs": [],
      "source": [
        "#@title Setup: Install Dependencies and Clone Repository\n",
        "!nvidia-smi || true\n",
        "!pip -q install tokenizers tqdm textstat\n",
        "\n",
        "import os, pathlib, subprocess, textwrap\n",
        "\n",
        "repo_root = pathlib.Path(\"/content/yoctoGPT\")\n",
        "if repo_root.exists():\n",
        "    print(\"Repo exists, pulling latest...\")\n",
        "    subprocess.run([\"git\", \"pull\"], cwd=repo_root, check=False)\n",
        "else:\n",
        "    subprocess.run(\n",
        "        [\n",
        "            \"git\",\n",
        "            \"clone\",\n",
        "            \"https://github.com/yhilpisch/yoctoGPT.git\",\n",
        "            str(repo_root),\n",
        "        ],\n",
        "        check=False,\n",
        "    )\n",
        "os.chdir(repo_root)\n",
        "\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    !pip -q install -r requirements.txt || true\n",
        "\n",
        "data_dir = pathlib.Path(\"data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "txts = list(data_dir.glob(\"*.txt\"))\n",
        "if not txts:\n",
        "    sample = textwrap.dedent('''\n",
        "    Philosophy is the study of general and fundamental questions,\n",
        "    such as those about existence, reason, knowledge, values, mind,\n",
        "    and language. It often poses questions rather than providing\n",
        "    answers, inviting us to think.\n",
        "    ''').strip()\n",
        "    (data_dir / \"philosophy.txt\").write_text(sample, encoding=\"utf-8\")\n",
        "    print(\"Created sample data/philosophy.txt\")\n",
        "else:\n",
        "    names = [p.name for p in txts][:5]\n",
        "    print(f\"Found {len(txts)} text files in data/: {names}\")"
      ],
      "id": "setup-code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "token-intro"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "Unlike character-level models, token-level models use more sophisticated encodings (like Byte Pair Encoding) that group common character sequences into single tokens. This allows the model to handle larger vocabularies and see more \"meaningful\" chunks of text at each step."
      ],
      "id": "token-intro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "token-code"
      },
      "outputs": [],
      "source": [
        "#@title Prepare Token-level Dataset\n",
        "!python -m scripts.prepare_tokenizer \\\n",
        "  --all_txt_in_dir \\\n",
        "  --text_dir data \\\n",
        "  --out_dir data/token \\\n",
        "  --vocab_size 8000 \\\n",
        "  --random_split \\\n",
        "  --split_seed 1337"
      ],
      "id": "token-code"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pick-hyperparams"
      },
      "outputs": [],
      "source": [
        "#@title Pick L4-friendly block_size/batch_size\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "train_path = Path('data/token/train.bin')\n",
        "val_path = Path('data/token/val.bin')\n",
        "train_tokens = int(np.fromfile(train_path, dtype=np.int32).shape[0])\n",
        "val_tokens = int(np.fromfile(val_path, dtype=np.int32).shape[0])\n",
        "min_tokens = min(train_tokens, val_tokens)\n",
        "\n",
        "block_candidates = [512, 384, 256, 192, 128, 96, 64, 48, 32, 24, 16]\n",
        "block_size = next((b for b in block_candidates if min_tokens > b + 2), max(8, min_tokens - 2))\n",
        "\n",
        "target_tokens = min(32768, max(2048, min_tokens))\n",
        "batch_size = max(1, min(96, target_tokens // block_size))\n",
        "\n",
        "print(f'Using block_size={block_size}, batch_size={batch_size}')\n"
      ],
      "id": "pick-hyperparams"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-intro"
      },
      "source": [
        "### Training\n",
        "\n",
        "We use `gpt_fast` with bf16 for efficient token-level training on Colab L4."
      ],
      "id": "training-intro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-code"
      },
      "outputs": [],
      "source": [
        "#@title Train (gpt_fast) on Colab L4\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path('/content/drive/MyDrive/yocto/checkpoints/colab_fast')\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "!python -m yoctoGPT.train \\\n",
        "--mode token \\\n",
        "--data_dir data/token \\\n",
        "--tokenizer_path data/token/tokenizer.json \\\n",
        "--ckpt_dir {CKPT_DIR} \\\n",
        "--model_type gpt_fast \\\n",
        "--device cuda \\\n",
        "--n_layer 6 --n_head 6 --n_embd 384 \\\n",
        "--block_size 512 --batch_size 128 \\\n",
        "--dropout 0.12 --weight_decay 0.08 \\\n",
        "--tie_weights --label_smoothing 0.05 \\\n",
        "--amp --amp_dtype bf16 \\\n",
        "--auto_microbatch \\\n",
        "--eval_interval 250 --eval_iters 50 \\\n",
        "--cosine_lr --warmup_iters 300 \\\n",
        "--min_lr 1e-5 --lr 1.8e-4 \\\n",
        "--max_iters 2000 \\\n",
        "--ema --ema_decay 0.999\n"
      ],
      "id": "training-code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sampling-intro"
      },
      "source": [
        "### Sampling and Resuming\n",
        "\n",
        "Generate text using the BPE tokenizer and learn how to resume training from your checkpoints."
      ],
      "id": "sampling-intro"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sampling-code"
      },
      "outputs": [],
      "source": [
        "#@title Sample Continuation\n",
        "output_token = !python -m yoctoGPT.sampler \\\n",
        "--mode token \\\n",
        "--ckpt {CKPT_DIR}/best.pt \\\n",
        "--tokenizer_path data/token/tokenizer.json \\\n",
        "--prompt \"In the beginning, philosophy sought to\" \\\n",
        "--max_new_tokens 120 \\\n",
        "--temperature 0.9 --top_k 30 --top_p 0.9\n",
        "\n",
        "generated_text_token = '\\n'.join(output_token)\n",
        "print(generated_text_token)\n"
      ],
      "id": "sampling-code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "readability-intro-03"
      },
      "id": "readability-intro-03",
      "source": [
        "### Readability Assessment\n",
        "\n",
        "Same readability scoring as Notebook 01, so token and char experiments can be compared on a common quality proxy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "readability-code-03"
      },
      "id": "readability-code-03",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Analyze Generated Text Readability\n",
        "from textstat import textstat\n",
        "\n",
        "def readability_scores(text: str) -> dict:\n",
        "    return {\n",
        "        'Flesch Reading Ease': textstat.flesch_reading_ease(text),\n",
        "        'Flesch-Kincaid Grade': textstat.flesch_kincaid_grade(text),\n",
        "        'Dale-Chall Score': textstat.dale_chall_readability_score(text),\n",
        "        'Text Standard': textstat.text_standard(text, float_output=False),\n",
        "    }\n",
        "\n",
        "scores = readability_scores(generated_text_token)\n",
        "for metric, value in scores.items():\n",
        "    print(f'{metric:20}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "resume-code"
      },
      "outputs": [],
      "source": [
        "#@title Resume training from best.pt (lower LR + early stopping)\n",
        "best = CKPT_DIR / 'best.pt'\n",
        "if not best.exists():\n",
        "    print('No best.pt found; skipping resume cell.')\n",
        "else:\n",
        "    !python -m yoctoGPT.train \\\n",
        "      --mode token \\\n",
        "      --data_dir data/token \\\n",
        "      --tokenizer_path data/token/tokenizer.json \\\n",
        "      --ckpt_dir {CKPT_DIR} \\\n",
        "      --resume {best} \\\n",
        "      --model_type gpt_fast \\\n",
        "      --device cuda \\\n",
        "      --n_layer 6 --n_head 6 --n_embd 384 \\\n",
        "      --block_size 512 --batch_size 128 \\\n",
        "      --dropout 0.12 \\\n",
        "      --tie_weights \\\n",
        "      --amp --amp_dtype bf16 \\\n",
        "      --auto_microbatch \\\n",
        "      --cosine_lr --warmup_iters 80 \\\n",
        "      --lr 8e-5 --max_iters 1200 \\\n",
        "      --eval_interval 300 --eval_iters 80 \\\n",
        "      --early_stopping_patience 3 \\\n",
        "      --early_stopping_min_delta 0.01\n"
      ],
      "id": "resume-code"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercises"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1. **Vocabulary Size**: Experiment with a larger `vocab_size` (e.g., 16,000) during tokenization. How does it affect the model's parameter count and training speed?\n",
        "2. **Different Architectures**: Try training with `--model_type gpt` or `gpt_plus` and compare the throughput (tokens per second) reported in the logs.\n",
        "3. **Hyperparameter Tuning**: Adjust `dropout` and `lr` to see if you can achieve a lower validation loss in fewer iterations."
      ],
      "id": "exercises"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tae-footer"
      },
      "source": [
        "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
      ],
      "id": "tae-footer"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}