{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-logo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yoctoGPT \u2014 Minimal GPT from Scratch\n",
    "\n",
    "## _Advanced Training \u2014 Token-level Training_\n",
    "\n",
    "**&copy; Dr. Yves J. Hilpisch**<br>AI-Powered by OpenAI & Gemini."
   ],
   "id": "tae-header"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- **Goal**: Train a GPT model using a Byte Pair Encoding (BPE) tokenizer.\n",
    "- **Hardware**: Optimized for Google Colab T4 (~15GB VRAM).\n",
    "- **Persistence**: Mounts Google Drive for persistent checkpoint storage."
   ],
   "id": "how-to-use"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "1. **Setup**: Install dependencies, clone repo, and mount Google Drive.\n",
    "2. **Tokenization**: Train a BPE tokenizer and encode the corpus into subword tokens.\n",
    "3. **Training**: Train a model using the `gpt_fast` architecture for maximum throughput.\n",
    "4. **Sampling & Resume**: Generate text using the tokenizer and resume from checkpoints."
   ],
   "id": "roadmap"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8Ghrt7f8eML"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive for checkpoint storage (repo stays local)\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_fast\")\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Checkpoints dir:\", CKPT_DIR)"
   ],
   "id": "mount-drive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wjDfe9n8eML"
   },
   "outputs": [],
   "source": [
    "#@title Setup: Install Dependencies and Clone Repository\n",
    "!nvidia-smi || true\n",
    "!pip -q install tokenizers tqdm\n",
    "\n",
    "import os, pathlib, subprocess, textwrap\n",
    "\n",
    "repo_root = pathlib.Path(\"/content/yoctoGPT\")\n",
    "if repo_root.exists():\n",
    "    print(\"Repo exists, pulling latest...\")\n",
    "    subprocess.run([\"git\", \"pull\"], cwd=repo_root, check=False)\n",
    "else:\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"https://github.com/yhilpisch/yoctoGPT.git\",\n",
    "            str(repo_root),\n",
    "        ],\n",
    "        check=False,\n",
    "    )\n",
    "os.chdir(repo_root)\n",
    "\n",
    "if os.path.exists(\"requirements.txt\"):\n",
    "    !pip -q install -r requirements.txt || true\n",
    "\n",
    "data_dir = pathlib.Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "txts = list(data_dir.glob(\"*.txt\"))\n",
    "if not txts:\n",
    "    sample = textwrap.dedent('''\n",
    "    Philosophy is the study of general and fundamental questions,\n",
    "    such as those about existence, reason, knowledge, values, mind,\n",
    "    and language. It often poses questions rather than providing\n",
    "    answers, inviting us to think.\n",
    "    ''').strip()\n",
    "    (data_dir / \"philosophy.txt\").write_text(sample, encoding=\"utf-8\")\n",
    "    print(\"Created sample data/philosophy.txt\")\n",
    "else:\n",
    "    names = [p.name for p in txts][:5]\n",
    "    print(f\"Found {len(txts)} text files in data/: {names}\")"
   ],
   "id": "setup-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Unlike character-level models, token-level models use more sophisticated encodings (like Byte Pair Encoding) that group common character sequences into single tokens. This allows the model to handle larger vocabularies and see more \"meaningful\" chunks of text at each step."
   ],
   "id": "token-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02itw5QS8eMM"
   },
   "outputs": [],
   "source": [
    "#@title Prepare Token-level Dataset\n",
    "!python -m scripts.prepare_tokenizer \\\n",
    "  --all_txt_in_dir \\\n",
    "  --text_dir data \\\n",
    "  --out_dir data/token \\\n",
    "  --vocab_size 8000 \\\n",
    "  --random_split \\\n",
    "  --split_seed 1337"
   ],
   "id": "token-code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uesjJtGn8eMM"
   },
   "outputs": [],
   "source": [
    "#@title Pick safe block_size/batch_size\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "train_path = Path(\"data/token/train.bin\")\n",
    "val_path = Path(\"data/token/val.bin\")\n",
    "train_tokens = int(np.fromfile(train_path, dtype=np.int32).shape[0])\n",
    "val_tokens = int(np.fromfile(val_path, dtype=np.int32).shape[0])\n",
    "min_tokens = min(train_tokens, val_tokens)\n",
    "\n",
    "block_candidates = [512, 384, 256, 192, 128, 96, 64, 48, 32, 24, 16]\n",
    "block_size = next(\n",
    "    (b for b in block_candidates if min_tokens > b + 2),\n",
    "    max(8, min_tokens - 2),\n",
    ")\n",
    "\n",
    "target_tokens = min(6000, max(512, min_tokens))\n",
    "batch_size = max(1, min(24, target_tokens // block_size))\n",
    "\n",
    "print(f\"Using block_size={block_size}, batch_size={batch_size}\")"
   ],
   "id": "pick-hyperparams"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We use the `gpt_fast` architecture, which leverages PyTorch's Scaled Dot-Product Attention (SDPA) for high performance on modern GPUs like the Tesla T4."
   ],
   "id": "training-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PebQYJ1s8eMM"
   },
   "outputs": [],
   "source": [
    "#@title Train (gpt_fast) on Colab T4\n",
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_fast\")\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "!python -m yoctoGPT.train \\\n",
    "  --mode token \\\n",
    "  --data_dir data/token \\\n",
    "  --tokenizer_path data/token/tokenizer.json \\\n",
    "  --ckpt_dir {CKPT_DIR} \\\n",
    "  --model_type gpt_fast \\\n",
    "  --device cuda \\\n",
    "  --n_layer 6 --n_head 6 --n_embd 384 \\\n",
    "  --block_size {block_size} --batch_size {batch_size} \\\n",
    "  --dropout 0.15 --weight_decay 0.08 \\\n",
    "  --tie_weights --label_smoothing 0.05 \\\n",
    "  --amp --amp_dtype bf16 \\\n",
    "  --grad_accum_steps 2 \\\n",
    "  --activation_checkpointing --auto_microbatch \\\n",
    "  --eval_interval 400 --eval_iters 200 \\\n",
    "  --cosine_lr --warmup_iters 400 \\\n",
    "  --min_lr 1e-5 --lr 2e-4 \\\n",
    "  --max_iters 1500 \\\n",
    "  --ema --ema_decay 0.999"
   ],
   "id": "training-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling and Resuming\n",
    "\n",
    "Generate text using the BPE tokenizer and learn how to resume training from your checkpoints."
   ],
   "id": "sampling-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsDNfocH8eMM"
   },
   "outputs": [],
   "source": [
    "#@title Sample Continuation\n",
    "!python -m yoctoGPT.sampler \\\n",
    "  --mode token \\\n",
    "  --ckpt {CKPT_DIR}/latest.pt \\\n",
    "  --tokenizer_path data/token/tokenizer.json \\\n",
    "  --prompt \"In the beginning, philosophy sought to\" \\\n",
    "  --max_new_tokens 200 \\\n",
    "  --temperature 0.9 --top_k 40 --top_p 0.95"
   ],
   "id": "sampling-code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj1nopCe8eMN"
   },
   "outputs": [],
   "source": [
    "#@title Resume training from latest.pt\n",
    "latest = CKPT_DIR / \"latest.pt\"\n",
    "if not latest.exists():\n",
    "    print(\"No latest.pt found; skipping resume cell.\")\n",
    "else:\n",
    "    !python -m yoctoGPT.train \\\n",
    "      --mode token \\\n",
    "      --data_dir data/token \\\n",
    "      --tokenizer_path data/token/tokenizer.json \\\n",
    "      --ckpt_dir {CKPT_DIR} \\\n",
    "      --resume {latest} \\\n",
    "      --model_type gpt_fast \\\n",
    "      --device cuda \\\n",
    "      --n_layer 6 --n_head 6 --n_embd 384 \\\n",
    "      --block_size {block_size} --batch_size {batch_size} \\\n",
    "      --amp --amp_dtype bf16 \\\n",
    "      --grad_accum_steps 2 \\\n",
    "      --activation_checkpointing --auto_microbatch \\\n",
    "      --max_iters 1800"
   ],
   "id": "resume-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Vocabulary Size**: Experiment with a larger `vocab_size` (e.g., 16,000) during tokenization. How does it affect the model's parameter count and training speed?\n",
    "2. **Different Architectures**: Try training with `--model_type gpt` or `gpt_plus` and compare the throughput (tokens per second) reported in the logs.\n",
    "3. **Hyperparameter Tuning**: Adjust `dropout` and `lr` to see if you can achieve a lower validation loss in fewer iterations."
   ],
   "id": "exercises"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-footer"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
