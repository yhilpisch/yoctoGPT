{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzjMTt5d79a-"
      },
      "source": [
        "# yoctoGPT (char-level) on Google Colab (T4, ~15GB VRAM)\n",
        "\n",
        "This notebook mounts Google Drive for persistent checkpoints (repo stays\n",
        "local on Colab), prepares a **character-level** dataset from text in\n",
        "`data/`, trains a char-level GPT configuration sized for a Colab T4\n",
        "(~15GB), provides sampling examples, and includes a resume cell. It also\n",
        "adapts context/batch size when the corpus is tiny to avoid random index\n",
        "errors."
      ],
      "id": "wzjMTt5d79a-"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Zpkk_w79bA",
        "outputId": "1c597ee1-bd08-4d9d-83c3-27faa57f09af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Checkpoints dir: /content/drive/MyDrive/yocto/checkpoints/colab_char\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive for checkpoint storage (repo stays local)\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_char\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Checkpoints dir:\", CKPT_DIR)"
      ],
      "id": "e-Zpkk_w79bA"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYn2jfRw79bB",
        "outputId": "5ae729f5-60a9-40ea-db33-83fe207c152f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 14 12:59:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Python 3.12.12\n",
            "Found 16 text files in data/: ['political_economy.txt', 'meditations.txt', 'wealth.txt', 'socialism.txt', 'siddharta.txt']\n"
          ]
        }
      ],
      "source": [
        "#@title Setup: install deps and clone/update the repo locally\n",
        "!nvidia-smi || true\n",
        "!python -V\n",
        "!pip -q install tokenizers tqdm\n",
        "\n",
        "import os, pathlib, subprocess, textwrap\n",
        "\n",
        "repo_root = pathlib.Path(\"/content/yoctoGPT\")\n",
        "if repo_root.exists():\n",
        "    print(\"Repo exists, pulling latest...\")\n",
        "    subprocess.run([\"git\", \"pull\"], cwd=repo_root, check=False)\n",
        "else:\n",
        "    subprocess.run(\n",
        "        [\n",
        "            \"git\",\n",
        "            \"clone\",\n",
        "            \"https://github.com/yhilpisch/yoctoGPT.git\",\n",
        "            str(repo_root),\n",
        "        ],\n",
        "        check=False,\n",
        "    )\n",
        "os.chdir(repo_root)\n",
        "\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    !pip -q install -r requirements.txt || true\n",
        "\n",
        "data_dir = pathlib.Path(\"data\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "txts = list(data_dir.glob(\"*.txt\"))\n",
        "if not txts:\n",
        "    sample = textwrap.dedent('''\n",
        "    Philosophy is the study of general and fundamental questions,\n",
        "    such as those about existence, reason, knowledge, values, mind,\n",
        "    and language. It often poses questions rather than providing\n",
        "    answers, inviting us to think.\n",
        "    ''').strip()\n",
        "    (data_dir / \"philosophy.txt\").write_text(sample, encoding=\"utf-8\")\n",
        "    print(\"Created sample data/philosophy.txt\")\n",
        "else:\n",
        "    names = [p.name for p in txts][:5]\n",
        "    print(f\"Found {len(txts)} text files in data/: {names}\")"
      ],
      "id": "mYn2jfRw79bB"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "425J9Hmm79bB",
        "outputId": "05ff0ac5-9662-4442-81ba-a704fa1722e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Including: data/anticipations.txt\n",
            "Including: data/capitalism.txt\n",
            "Including: data/economist.txt\n",
            "Including: data/economy.txt\n",
            "Including: data/essentials.txt\n",
            "Including: data/insights.txt\n",
            "Including: data/meditations.txt\n",
            "Including: data/philosophy.txt\n",
            "Including: data/political_economy.txt\n",
            "Including: data/principles.txt\n",
            "Including: data/psycho_stocks.txt\n",
            "Including: data/religion.txt\n",
            "Including: data/siddharta.txt\n",
            "Including: data/socialism.txt\n",
            "Including: data/stocks.txt\n",
            "Including: data/wealth.txt\n",
            "Wrote merged corpus to data/all_texts_char.txt\n"
          ]
        }
      ],
      "source": [
        "#@title Char data: prepare char-level dataset from a text file\n",
        "# Adjust `--text_path` if you want a different source file.\n",
        "data_dir = pathlib.Path(\"data\")\n",
        "texts = []\n",
        "for p in sorted(data_dir.glob(\"*.txt\")):\n",
        "    print(\"Including:\", p)\n",
        "    texts.append(p.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "merged_path = data_dir / \"all_texts_char.txt\"\n",
        "merged_path.write_text(\"\\n\\n\".join(texts), encoding=\"utf-8\")\n",
        "print(\"Wrote merged corpus to\", merged_path)"
      ],
      "id": "425J9Hmm79bB"
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m scripts.prepare_char_data \\\n",
        "  --text_path data/all_texts_char.txt \\\n",
        "  --out_dir data/char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HW76YbJ9OZn",
        "outputId": "d0951d0e-2084-42d5-9008-7ea93af962da"
      },
      "id": "1HW76YbJ9OZn",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 9619983 train and 1068886 val tokens.\n",
            "Vocab size: 211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpPpJAYs79bC",
        "outputId": "3ce33862-2287-45b0-eb88-a87ac629ee77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tokens: 9619983, Val tokens: 1068886, min_tokens: 1068886\n",
            "Using block_size=512, batch_size=11\n"
          ]
        }
      ],
      "source": [
        "#@title Pick safe block_size/batch_size for this char corpus\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "train_path = Path(\"data/char/train.bin\")\n",
        "val_path = Path(\"data/char/val.bin\")\n",
        "train_tokens = int(np.fromfile(train_path, dtype=np.int32).shape[0])\n",
        "val_tokens = int(np.fromfile(val_path, dtype=np.int32).shape[0])\n",
        "min_tokens = min(train_tokens, val_tokens)\n",
        "\n",
        "if min_tokens <= 4:\n",
        "    raise SystemExit(\n",
        "        \"Dataset too small. Add more text to data/ and \"\n",
        "        \"rerun the char data preparation.\"\n",
        "    )\n",
        "\n",
        "block_candidates = [512, 384, 256, 192, 128, 96, 64, 48, 32, 24, 16]\n",
        "block_size = next(\n",
        "    (b for b in block_candidates if min_tokens > b + 2),\n",
        "    max(8, min_tokens - 2),\n",
        ")\n",
        "\n",
        "target_tokens = min(16000, max(512, min_tokens))\n",
        "batch_size = max(1, min(24, target_tokens // block_size))\n",
        "\n",
        "print(\n",
        "    f\"Train tokens: {train_tokens}, Val tokens: {val_tokens}, \"\n",
        "    f\"min_tokens: {min_tokens}\"\n",
        ")\n",
        "print(f\"Using block_size={block_size}, batch_size={batch_size}\")"
      ],
      "id": "xpPpJAYs79bC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkrYZCGk79bC"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Get an auto-recommended command for this GPU (T4 ~15GB)\n",
        "!python -m scripts.recommend_training \\\n",
        "  --mode char \\\n",
        "  --data_dir data/char \\\n",
        "  --ckpt_dir {CKPT_DIR} \\\n",
        "  --priority speed \\\n",
        "  --device cuda \\\n",
        "  --device_mem_gb 15"
      ],
      "id": "WkrYZCGk79bC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEubi6-C79bC"
      },
      "source": [
        "## Training\n",
        "\n",
        "We use a char-level configuration sized for a Colab T4. If you hit OOM,\n",
        "lower `batch_size` or `block_size`; if you have headroom, you can nudge\n",
        "them upward. The auto-picked block/batch above avoids tiny-corpus indexing\n",
        "errors."
      ],
      "id": "zEubi6-C79bC"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptXUtIui79bC",
        "outputId": "c7991478-4456-4fd1-879c-b951ab7fc201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training: 100% 2000/2000 [02:57<00:00, 11.27it/s, train_loss=2.48, val_loss=2.56]\n",
            "{'final_val_loss': 2.563058043718338}\n"
          ]
        }
      ],
      "source": [
        "#@title Train (char-level GPT) on Colab T4\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_char\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "!python -m yoctoGPT.train \\\n",
        "  --mode char \\\n",
        "  --data_dir data/char \\\n",
        "  --ckpt_dir {CKPT_DIR} \\\n",
        "  --model_type gpt_fast \\\n",
        "  --device cuda \\\n",
        "  --n_layer 8 --n_head 8 --n_embd 512 \\\n",
        "  --block_size {block_size} --batch_size {batch_size} \\\n",
        "  --dropout 0.1 --weight_decay 0.1 \\\n",
        "  --tie_weights --label_smoothing 0.05 \\\n",
        "  --eval_interval 800 --eval_iters 100 \\\n",
        "  --cosine_lr --warmup_iters 400 \\\n",
        "  --min_lr 1e-5 --lr 2e-4 \\\n",
        "  --max_iters 6000 \\\n",
        "  --ema --ema_decay 0.999"
      ],
      "id": "ptXUtIui79bC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyqIBLHZ79bD"
      },
      "source": [
        "## Sampling examples\n",
        "\n",
        "Generate text from the latest char-level checkpoint. Adjust temperature and\n",
        "max length for different styles and lengths."
      ],
      "id": "kyqIBLHZ79bD"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3xSLf-79bD",
        "outputId": "5fd889fc-762e-43d1-dde7-6e892e491458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Philosophy iss at conty thall orole the prostions\n",
            "sthermat the suse site at ond wathe bus ardof the of man by prepptuctses. The whin ild\n",
            "hessis of sthe pe the duceresesed on the thesas be of f thesucced\n",
            "be ffon oucoss its care then be wacoung, a cin anghis erthe indedelintics\n",
            "ined the lo dererestins the is the i\n"
          ]
        }
      ],
      "source": [
        "#@title Sample 1: simple continuation\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_char\")\n",
        "\n",
        "!python -m yoctoGPT.sampler \\\n",
        "  --mode char \\\n",
        "  --ckpt {CKPT_DIR}/latest.pt \\\n",
        "  --vocab_path data/char/vocab.json \\\n",
        "  --prompt \"Philosophy is\" \\\n",
        "  --max_new_tokens 300 \\\n",
        "  --temperature 0.8 --top_k 50 --top_p 0.95"
      ],
      "id": "7T3xSLf-79bD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3eeDR7_79bD"
      },
      "outputs": [],
      "source": [
        "#@title Sample 2: multi-line prompt\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_char\")\n",
        "\n",
        "!python -m yoctoGPT.sampler \\\n",
        "  --mode char \\\n",
        "  --ckpt {CKPT_DIR}/latest.pt \\\n",
        "  --vocab_path data/char/vocab.json \\\n",
        "  --prompt \"Question: What is wisdom?\\nAnswer:\" \\\n",
        "  --max_new_tokens 300 \\\n",
        "  --temperature 0.9 --top_k 40 --top_p 0.95"
      ],
      "id": "u3eeDR7_79bD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q6oMMiN79bE"
      },
      "source": [
        "## Resume training\n",
        "\n",
        "Resume from the latest checkpoint to continue char-level training for\n",
        "additional steps. `--max_iters` is interpreted as extra steps beyond the\n",
        "checkpointed iteration count."
      ],
      "id": "5q6oMMiN79bE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56YUIamA79bE",
        "outputId": "89c88173-ba45-48fe-e369-ce76c3cc5e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed from /content/drive/MyDrive/yocto/checkpoints/colab_char/latest.pt at iter 2000\n",
            "training:  24% 2875/12000 [01:14<08:15, 18.41it/s, train_loss=2.37, val_loss=2.44]"
          ]
        }
      ],
      "source": [
        "#@title Resume training from latest.pt (additional 1000 steps)\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_char\")\n",
        "latest = CKPT_DIR / \"latest.pt\"\n",
        "if not latest.exists():\n",
        "    raise SystemExit(\n",
        "        \"No latest.pt found in CKPT_DIR; run initial training first.\"\n",
        "    )\n",
        "\n",
        "!python -m yoctoGPT.train \\\n",
        "  --mode char \\\n",
        "  --data_dir data/char \\\n",
        "  --ckpt_dir {CKPT_DIR} \\\n",
        "  --resume {latest} \\\n",
        "  --model_type gpt_fast \\\n",
        "  --device cuda \\\n",
        "  --n_layer 8 --n_head 8 --n_embd 512 \\\n",
        "  --block_size {block_size} --batch_size {batch_size} \\\n",
        "  --dropout 0.1 --weight_decay 0.1 \\\n",
        "  --tie_weights --label_smoothing 0.05 \\\n",
        "  --eval_interval 800 --eval_iters 100 \\\n",
        "  --cosine_lr --warmup_iters 400 \\\n",
        "  --min_lr 1e-5 --lr 2e-4 \\\n",
        "  --max_iters 6000 \\\n",
        "  --ema --ema_decay 0.999"
      ],
      "id": "56YUIamA79bE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3e6uVMl79bE"
      },
      "source": [
        "Optionally, inspect the last lines of the training metrics CSV to monitor\n",
        "progress."
      ],
      "id": "D3e6uVMl79bE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yy9ei9c79bE"
      },
      "outputs": [],
      "source": [
        "#@title Inspect metrics\n",
        "from pathlib import Path\n",
        "\n",
        "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_char\")\n",
        "\n",
        "!tail -n 20 {CKPT_DIR}/metrics.csv || true"
      ],
      "id": "7yy9ei9c79bE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}