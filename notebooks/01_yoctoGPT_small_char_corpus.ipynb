{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-logo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yoctoGPT \u2014 Minimal GPT from Scratch\n",
    "\n",
    "## _Getting Started \u2014 Character-level Training_\n",
    "\n",
    "**&copy; Dr. Yves J. Hilpisch**<br>AI-Powered by OpenAI & Gemini."
   ],
   "id": "tae-header"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- **Goal**: Train a tiny GPT model on a character-level corpus in minutes.\n",
    "- **Hardware**: Designed for Google Colab T4 (free tier) or better.\n",
    "- **Approach**: We use `yoctoGPT`, a minimal PyTorch implementation focused on clarity."
   ],
   "id": "how-to-use"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "1. **Setup**: Install dependencies and clone the `yoctoGPT` repository.\n",
    "2. **Data**: Prepare a character-level dataset from a philosophical text.\n",
    "3. **Training**: Execute a short training run with a small model parametrization.\n",
    "4. **Sampling**: Generate text from the trained checkpoint to see the model in action."
   ],
   "id": "roadmap"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Environment and Dependencies\n",
    "\n",
    "We first check the GPU availability and install the necessary requirements."
   ],
   "id": "setup-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check GPU and Install Dependencies\n",
    "!nvidia-smi\n",
    "!pip -q install tokenizers tqdm textstat\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "# Clone the repository if it doesn't exist\n",
    "repo_root = pathlib.Path(\"/content/yoctoGPT\")\n",
    "if not repo_root.exists():\n",
    "    print(\"Cloning yoctoGPT repository...\")\n",
    "    subprocess.run([\n",
    "        \"git\", \"clone\",\n",
    "        \"https://github.com/yhilpisch/yoctoGPT.git\",\n",
    "        str(repo_root)\n",
    "    ])\n",
    "os.chdir(repo_root)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ],
   "id": "setup-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We use a character-level encoding where each character (letter, digit, punctuation) is assigned a unique integer ID. This is the simplest way to start building a language model."
   ],
   "id": "data-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Prepare Character-level Data\n",
    "# We use 'philosophy.txt' as a small, clean corpus (~240KB)\n",
    "!python -m scripts.prepare_char_data \\\n",
    "    --text_path data/philosophy.txt \\\n",
    "    --out_dir data/char_start"
   ],
   "id": "data-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "We initialize a small GPT model. Smaller models train faster and are perfect for learning the core mechanics without waiting for hours.\n",
    "\n",
    "**Configuration:**\n",
    "- `n_layer`: 2 (number of transformer blocks)\n",
    "- `n_head`: 4 (number of attention heads)\n",
    "- `n_embd`: 128 (embedding dimension)\n",
    "- `block_size`: 128 (context window length)"
   ],
   "id": "training-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train Tiny yoctoGPT\n",
    "# This automatically saves checkpoints (best.pt, latest.pt) to --ckpt_dir\n",
    "!python -m yoctoGPT.train \\\n",
    "    --mode char \\\n",
    "    --data_dir data/char_start \\\n",
    "    --ckpt_dir checkpoints/char_start \\\n",
    "    --n_layer 2 \\\n",
    "    --n_head 4 \\\n",
    "    --n_embd 128 \\\n",
    "    --block_size 128 \\\n",
    "    --batch_size 32 \\\n",
    "    --max_iters 500 \\\n",
    "    --lr 1e-3 \\\n",
    "    --eval_interval 100\n",
    "\n",
    "# Verify checkpoint creation\n",
    "!ls -lh checkpoints/char_start/latest.pt"
   ],
   "id": "training-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation (Sampling)\n",
    "\n",
    "Once trained, we can prompt the model to generate new text based on what it learned from the corpus."
   ],
   "id": "sampling-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Sample from the Model\n",
    "prompt = \"The meaning of life is\"\n",
    "output = !python -m yoctoGPT.sampler \\\n",
    "    --mode char \\\n",
    "    --ckpt checkpoints/char_start/latest.pt \\\n",
    "    --vocab_path data/char_start/vocab.json \\\n",
    "    --prompt \"{prompt}\" \\\n",
    "    --max_new_tokens 200\n",
    "\n",
    "generated_text = \"\\n\".join(output)\n",
    "print(generated_text)"
   ],
   "id": "sampling-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability Assessment\n",
    "\n",
    "We evaluate the quality and complexity of the generated text using standard readability metrics. This helps us understand if the model is producing coherent, human-like structures or just random character sequences.\n",
    "\n",
    "**Metric Meanings:**\n",
    "- **Flesch Reading Ease**: Higher scores indicate text that is easier to read (100 is very easy, 0 is very difficult).\n",
    "- **Flesch-Kincaid Grade**: Estimates the U.S. school grade level required to understand the text.\n",
    "- **Dale-Chall Score**: Uses a list of 3,000 familiar words to assess complexity (lower is easier).\n",
    "- **Text Standard**: A consensus metric that summarizes the estimated reading level (e.g., '8th Grade')."
   ],
   "id": "readability-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Analyze Generated Text Readability\n",
    "from textstat import textstat\n",
    "import json\n",
    "\n",
    "def readability_scores(text: str) -> dict:\n",
    "    return {\n",
    "        \"Flesch Reading Ease\": textstat.flesch_reading_ease(text),\n",
    "        \"Flesch-Kincaid Grade\": textstat.flesch_kincaid_grade(text),\n",
    "        \"Dale-Chall Score\": textstat.dale_chall_readability_score(text),\n",
    "        \"Text Standard\": textstat.text_standard(text, float_output=False),\n",
    "    }\n",
    "\n",
    "# Clean the output (remove prompt and potential status lines)\n",
    "# Assuming the sampler prints the full sequence including the prompt.\n",
    "scores = readability_scores(generated_text)\n",
    "\n",
    "print(\"Readability Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "for metric, value in scores.items():\n",
    "    print(f\"{metric:20}: {value}\")"
   ],
   "id": "readability-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume Training\n",
    "\n",
    "You can continue training from a saved checkpoint to further improve the model's performance. This uses the `latest.pt` file saved in your checkpoint directory."
   ],
   "id": "resume-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Resume training from latest.pt\n",
    "latest_path = \"checkpoints/char_start/latest.pt\"\n",
    "if os.path.exists(latest_path):\n",
    "    !python -m yoctoGPT.train \\\n",
    "        --mode char \\\n",
    "        --data_dir data/char_start \\\n",
    "        --ckpt_dir checkpoints/char_start \\\n",
    "        --resume {latest_path} \\\n",
    "        --n_layer 2 \\\n",
    "        --n_head 4 \\\n",
    "        --n_embd 128 \\\n",
    "        --block_size 128 \\\n",
    "        --max_iters 500\n",
    "else:\n",
    "    print(f\"No checkpoint found at {latest_path}. Run the initial training first.\")"
   ],
   "id": "resume-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Experiment with Context**: Increase the `block_size` to 256 and see how it affects memory usage (VRAM) and training speed.\n",
    "2. **Deeper Model**: Change `n_layer` to 4 and observe the validation loss after 500 iterations. Does it improve?\n",
    "3. **Custom Corpus**: Upload your own `.txt` file to the `data/` folder and retrain the model on your own text."
   ],
   "id": "exercises"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-footer"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
