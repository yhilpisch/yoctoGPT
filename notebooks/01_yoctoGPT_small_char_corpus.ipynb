{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tae-logo"
   },
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-logo"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tae-header"
   },
   "source": [
    "# yoctoGPT — Minimal GPT from Scratch\n",
    "\n",
    "## _Getting Started — Character-level Training_\n",
    "\n",
    "**&copy; Dr. Yves J. Hilpisch**<br>AI-Powered by OpenAI & Gemini."
   ],
   "id": "tae-header"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "how-to-use"
   },
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- **Goal**: Train a tiny GPT model on a character-level corpus in minutes.\n",
    "- **Hardware**: Designed for Google Colab L4 (free tier) or better.\n",
    "- **Approach**: We use `yoctoGPT`, a minimal PyTorch implementation focused on clarity."
   ],
   "id": "how-to-use"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roadmap"
   },
   "source": [
    "### Roadmap\n",
    "\n",
    "1. **Setup**: Install dependencies and clone the `yoctoGPT` repository.\n",
    "2. **Data**: Prepare a character-level dataset from a philosophical text.\n",
    "3. **Training**: Execute a short training run with a small model parametrization.\n",
    "4. **Sampling**: Generate text from the trained checkpoint to see the model in action."
   ],
   "id": "roadmap"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-intro"
   },
   "source": [
    "### Setup: Environment and Dependencies\n",
    "\n",
    "We first check the GPU availability and install the necessary requirements."
   ],
   "id": "setup-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-code"
   },
   "outputs": [],
   "source": [
    "#@title Check GPU and Install Dependencies\n",
    "!nvidia-smi\n",
    "!pip -q install tokenizers tqdm textstat\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "# Clone the repository if it doesn't exist\n",
    "repo_root = pathlib.Path(\"/content/yoctoGPT\")\n",
    "if not repo_root.exists():\n",
    "    print(\"Cloning yoctoGPT repository...\")\n",
    "    subprocess.run([\n",
    "        \"git\", \"clone\",\n",
    "        \"https://github.com/yhilpisch/yoctoGPT.git\",\n",
    "        str(repo_root)\n",
    "    ])\n",
    "os.chdir(repo_root)\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ],
   "id": "setup-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-intro"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We use a character-level encoding where each character (letter, digit, punctuation) is assigned a unique integer ID. This is the simplest way to start building a language model."
   ],
   "id": "data-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-code"
   },
   "outputs": [],
   "source": [
    "#@title Prepare Character-level Data\n",
    "!python -m scripts.prepare_char_data \\\n",
    "    --text_path data/philosophy.txt \\\n",
    "    --out_dir data/char_start \\\n",
    "    --sanitize_chars ascii \\\n",
    "    --no_punctuation \\\n",
    "    --keep_period \\\n",
    "    --collapse_whitespace \\\n",
    "    --lowercase"
   ],
   "id": "data-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-intro"
   },
   "source": [
    "### Model Training\n",
    "\n",
    "We initialize a small GPT model. Smaller models train faster and are perfect for learning the core mechanics without waiting for hours.\n",
    "\n",
    "**Configuration:**\n",
    "- `n_layer`: 2 (number of transformer blocks)\n",
    "- `n_head`: 4 (number of attention heads)\n",
    "- `n_embd`: 128 (embedding dimension)\n",
    "- `block_size`: 128 (context window length)"
   ],
   "id": "training-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-code"
   },
   "outputs": [],
   "source": [
    "#@title Train Tiny yoctoGPT\n",
    "# This automatically saves checkpoints (best.pt, latest.pt) to --ckpt_dir\n",
    "!python -m yoctoGPT.train \\\n",
    "    --mode char \\\n",
    "    --data_dir data/char_start \\\n",
    "    --ckpt_dir checkpoints/char_start \\\n",
    "    --n_layer 2 \\\n",
    "    --n_head 4 \\\n",
    "    --n_embd 128 \\\n",
    "    --block_size 128 \\\n",
    "    --batch_size 32 \\\n",
    "    --max_iters 500 \\\n",
    "    --lr 1e-3 \\\n",
    "    --eval_interval 100\n",
    "\n",
    "# Verify checkpoint creation\n",
    "!ls -lh checkpoints/char_start/latest.pt"
   ],
   "id": "training-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "resume-intro"
   },
   "source": [
    "### Resume Training\n",
    "\n",
    "You can continue training from a saved checkpoint to further improve the model's performance. This uses the `latest.pt` file saved in your checkpoint directory."
   ],
   "id": "resume-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "resume-code"
   },
   "outputs": [],
   "source": [
    "#@title Resume training from latest.pt\n",
    "latest_path = \"checkpoints/char_start/latest.pt\"\n",
    "if os.path.exists(latest_path):\n",
    "    !python -m yoctoGPT.train \\\n",
    "        --mode char \\\n",
    "        --data_dir data/char_start \\\n",
    "        --ckpt_dir checkpoints/char_start \\\n",
    "        --resume {latest_path} \\\n",
    "        --n_layer 2 \\\n",
    "        --n_head 4 \\\n",
    "        --n_embd 128 \\\n",
    "        --block_size 128 \\\n",
    "        --max_iters 2500\n",
    "else:\n",
    "    print(f\"No checkpoint found at {latest_path}. Run the initial training first.\")"
   ],
   "id": "resume-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sampling-intro"
   },
   "source": [
    "### Text Generation (Sampling)\n",
    "\n",
    "Once trained, we can prompt the model to generate new text based on what it learned from the corpus."
   ],
   "id": "sampling-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sampling-code"
   },
   "outputs": [],
   "source": [
    "#@title Sample from the Model\n",
    "prompt = \"the meaning of life is\"\n",
    "output = !python -m yoctoGPT.sampler \\\n",
    "    --mode char \\\n",
    "    --ckpt checkpoints/char_start/latest.pt \\\n",
    "    --vocab_path data/char_start/vocab.json \\\n",
    "    --prompt \"{prompt}\" \\\n",
    "    --max_new_tokens 200\n",
    "\n",
    "generated_text = \"\\n\".join(output)\n",
    "print(generated_text)"
   ],
   "id": "sampling-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF2oXku-TeHU"
   },
   "source": [
    "### Extended Training on the Same Corpus\n",
    "\n",
    "The first run above is intentionally tiny so you can see end-to-end behavior in a few minutes.\n",
    "This block keeps the same small character corpus (`philosophy.txt`) but trains a larger transformer from scratch with a more realistic configuration.\n",
    "\n",
    "Use the same sampling and readability cells to compare outputs and scores from the tiny run vs this deeper run.\n"
   ],
   "id": "wF2oXku-TeHU"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SZjSFS9STeHU"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Train deeper yoctoGPT on the same corpus\n",
    "!python -m yoctoGPT.train \\\n",
    "    --mode char \\\n",
    "    --data_dir data/char_start \\\n",
    "    --ckpt_dir checkpoints/char_deeper \\\n",
    "    --n_layer 6 \\\n",
    "    --n_head 8 \\\n",
    "    --n_embd 256 \\\n",
    "    --block_size 256 \\\n",
    "    --batch_size 32 \\\n",
    "    --max_iters 1500 \\\n",
    "    --lr 3e-4 \\\n",
    "    --eval_interval 500"
   ],
   "id": "SZjSFS9STeHU"
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Same idea, but better utilization of the L4 card\n",
    "!python -m yoctoGPT.train \\\n",
    "    --mode char \\\n",
    "    --data_dir data/char_start \\\n",
    "    --ckpt_dir checkpoints/char_deeper_l4_tuned \\\n",
    "    --model_type gpt_fast \\\n",
    "    --n_layer 8 \\\n",
    "    --n_head 8 \\\n",
    "    --n_embd 512 \\\n",
    "    --block_size 384 \\\n",
    "    --batch_size 128 \\\n",
    "    --max_iters 1500 \\\n",
    "    --lr 1.2e-4 \\\n",
    "    --amp \\\n",
    "    --amp_dtype bf16 \\\n",
    "    --eval_interval 250 \\\n",
    "    --eval_iters 30 \\\n",
    "    --auto_microbatch \\\n",
    "    --cosine_lr \\\n",
    "    --warmup_iters 100"
   ],
   "metadata": {
    "id": "JIU3vpLgb5OC"
   },
   "id": "JIU3vpLgb5OC",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Resume from best checkpoint (lower LR, early stopping)\n",
    "!python -m yoctoGPT.train \\\n",
    "    --mode char \\\n",
    "    --data_dir data/char_start \\\n",
    "    --ckpt_dir checkpoints/char_deeper_l4_tuned \\\n",
    "    --resume checkpoints/char_deeper_l4_tuned/best.pt \\\n",
    "    --model_type gpt_fast \\\n",
    "    --n_layer 8 \\\n",
    "    --n_head 8 \\\n",
    "    --n_embd 512 \\\n",
    "    --block_size 384 \\\n",
    "    --batch_size 128 \\\n",
    "    --max_iters 750 \\\n",
    "    --lr 2e-5 \\\n",
    "    --amp \\\n",
    "    --amp_dtype bf16 \\\n",
    "    --eval_interval 250 \\\n",
    "    --eval_iters 60 \\\n",
    "    --auto_microbatch \\\n",
    "    --cosine_lr \\\n",
    "    --warmup_iters 50 \\\n",
    "    --early_stopping_patience 3 \\\n",
    "    --early_stopping_min_delta 0.01"
   ],
   "metadata": {
    "id": "q3mA9w4omjg-"
   },
   "id": "q3mA9w4omjg-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "361SQNUGTeHU"
   },
   "source": [
    "### Text Generation: Deeper Model\n",
    "\n",
    "After training the larger transformer on the same corpus, you can sample from it\n",
    "to see whether the outputs become less gibberish-like. Use the same prompt so\n",
    "comparisons stay fair.\n"
   ],
   "id": "361SQNUGTeHU"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yg0p-uh4TeHU"
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Sample from the deeper model\n",
    "prompt = 'the meaning of life is'\n",
    "output_deeper = !python -m yoctoGPT.sampler \\\n",
    "--mode char \\\n",
    "--ckpt checkpoints/char_deeper_l4_tuned/latest.pt \\\n",
    "--vocab_path data/char_start/vocab.json \\\n",
    "--prompt '{prompt}' \\\n",
    "--max_new_tokens 200\n",
    "\n",
    "deeper_generated_text = '\\n'.join(output_deeper)\n",
    "print(deeper_generated_text)\n"
   ],
   "id": "Yg0p-uh4TeHU"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "readability-intro"
   },
   "source": [
    "### Readability Assessment\n",
    "\n",
    "We evaluate the quality and complexity of the generated text using standard readability metrics. This helps us understand if the model is producing coherent, human-like structures or just random character sequences.\n",
    "\n",
    "**Metric Meanings:**\n",
    "- **Flesch Reading Ease**: Higher scores indicate text that is easier to read (100 is very easy, 0 is very difficult).\n",
    "- **Flesch-Kincaid Grade**: Estimates the U.S. school grade level required to understand the text.\n",
    "- **Dale-Chall Score**: Uses a list of 3,000 familiar words to assess complexity (lower is easier).\n",
    "- **Text Standard**: A consensus metric that summarizes the estimated reading level (e.g., '8th Grade')."
   ],
   "id": "readability-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "readability-code"
   },
   "outputs": [],
   "source": [
    "#@title Analyze Generated Text Readability\n",
    "from textstat import textstat\n",
    "import json\n",
    "\n",
    "def readability_scores(text: str) -> dict:\n",
    "    return {\n",
    "        \"Flesch Reading Ease\": textstat.flesch_reading_ease(text),\n",
    "        \"Flesch-Kincaid Grade\": textstat.flesch_kincaid_grade(text),\n",
    "        \"Dale-Chall Score\": textstat.dale_chall_readability_score(text),\n",
    "        \"Text Standard\": textstat.text_standard(text, float_output=False),\n",
    "    }\n",
    "\n",
    "print(\"Readability Analysis (tiny model: checkpoints/char_start)\")\n",
    "print('-' * 40)\n",
    "scores_tiny = readability_scores(generated_text)\n",
    "for metric, value in scores_tiny.items():\n",
    "    print(f'{metric:20}: {value}')\n",
    "\n",
    "# If a deeper model sample exists, compare its scores as well\n",
    "if 'deeper_generated_text' in globals():\n",
    "    print(\"\\nReadability Analysis (deeper model: checkpoints/char_deeper_l4_tuned)\")\n",
    "    print('-' * 40)\n",
    "    scores_deeper = readability_scores(deeper_generated_text)\n",
    "    for metric, value in scores_deeper.items():\n",
    "        print(f'{metric:20}: {value}')\n"
   ],
   "id": "readability-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercises"
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Experiment with Context**: Increase the `block_size` to 256 and see how it affects memory usage (VRAM) and training speed.\n",
    "2. **Deeper Model**: Change `n_layer` to 4 and observe the validation loss after 500 iterations. Does it improve?\n",
    "3. **Custom Corpus**: Upload your own `.txt` file to the `data/` folder and retrain the model on your own text."
   ],
   "id": "exercises"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tae-footer"
   },
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-footer"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
