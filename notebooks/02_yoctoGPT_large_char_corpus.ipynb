{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-logo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yoctoGPT — Minimal GPT from Scratch\n",
    "\n",
    "## _Scaling Up — Character-level Training_\n",
    "\n",
    "**&copy; Dr. Yves J. Hilpisch**<br>AI-Powered by OpenAI & Gemini."
   ],
   "id": "tae-header"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- **Goal**: Train a larger character-level GPT model on a combined corpus.\n",
    "- **Hardware**: Optimized for Google Colab T4 (~15GB VRAM).\n",
    "- **Persistence**: Mounts Google Drive for persistent checkpoint storage."
   ],
   "id": "how-to-use"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "1. **Setup**: Install dependencies, clone repo, and mount Google Drive.\n",
    "2. **Data**: Merge multiple text files into a large character-level corpus.\n",
    "3. **Training**: Train a larger model (8 layers, 512 embedding dim) with optimized settings.\n",
    "4. **Sampling & Resume**: Sample from checkpoints and learn how to resume training."
   ],
   "id": "roadmap"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-Zpkk_w79bA"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive for checkpoint storage (repo stays local)\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_char\")\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Checkpoints dir:\", CKPT_DIR)"
   ],
   "id": "mount-drive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYn2jfRw79bB"
   },
   "outputs": [],
   "source": [
    "#@title Setup: Install Dependencies and Clone Repository\n",
    "!nvidia-smi || true\n",
    "!pip -q install tokenizers tqdm\n",
    "\n",
    "import os, pathlib, subprocess, textwrap\n",
    "\n",
    "repo_root = pathlib.Path(\"/content/yoctoGPT\")\n",
    "if repo_root.exists():\n",
    "    print(\"Repo exists, pulling latest...\")\n",
    "    subprocess.run([\"git\", \"pull\"], cwd=repo_root, check=False)\n",
    "else:\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"https://github.com/yhilpisch/yoctoGPT.git\",\n",
    "            str(repo_root),\n",
    "        ],\n",
    "        check=False,\n",
    "    )\n",
    "os.chdir(repo_root)\n",
    "\n",
    "if os.path.exists(\"requirements.txt\"):\n",
    "    !pip -q install -r requirements.txt || true\n",
    "\n",
    "data_dir = pathlib.Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "txts = list(data_dir.glob(\"*.txt\"))\n",
    "if not txts:\n",
    "    sample = textwrap.dedent('''\n",
    "    Philosophy is the study of general and fundamental questions,\n",
    "    such as those about existence, reason, knowledge, values, mind,\n",
    "    and language. It often poses questions rather than providing\n",
    "    answers, inviting us to think.\n",
    "    ''').strip()\n",
    "    (data_dir / \"philosophy.txt\").write_text(sample, encoding=\"utf-8\")\n",
    "    print(\"Created sample data/philosophy.txt\")\n",
    "else:\n",
    "    names = [p.name for p in txts][:5]\n",
    "    print(f\"Found {len(txts)} text files in data/: {names}\")"
   ],
   "id": "setup-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We merge all available text files in the `data/` directory to create a larger, more diverse training set for our character-level model."
   ],
   "id": "data-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "425J9Hmm79bB"
   },
   "outputs": [],
   "source": [
    "#@title Merge Texts and Prepare Char-level Dataset\n",
    "data_dir = pathlib.Path(\"data\")\n",
    "texts = []\n",
    "for p in sorted(data_dir.glob(\"*.txt\")):\n",
    "    print(\"Including:\", p)\n",
    "    texts.append(p.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "merged_path = data_dir / \"all_texts_char.txt\"\n",
    "merged_path.write_text(\"\\n\\n\".join(texts), encoding=\"utf-8\")\n",
    "print(\"Wrote merged corpus to\", merged_path)\n",
    "\n",
    "!python -m scripts.prepare_char_data \\\n",
    "  --text_path data/all_texts_char.txt \\\n",
    "  --out_dir data/char"
   ],
   "id": "data-code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpPpJAYs79bC"
   },
   "outputs": [],
   "source": [
    "#@title Pick L4-friendly block_size/batch_size\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "train_path = Path('data/char/train.bin')\n",
    "val_path = Path('data/char/val.bin')\n",
    "train_tokens = int(np.fromfile(train_path, dtype=np.int32).shape[0])\n",
    "val_tokens = int(np.fromfile(val_path, dtype=np.int32).shape[0])\n",
    "min_tokens = min(train_tokens, val_tokens)\n",
    "\n",
    "block_candidates = [512, 384, 256, 192, 128, 96, 64, 48, 32, 24, 16]\n",
    "block_size = next((b for b in block_candidates if min_tokens > b + 2), max(8, min_tokens - 2))\n",
    "\n",
    "target_tokens = min(49152, max(2048, min_tokens))\n",
    "batch_size = max(1, min(128, target_tokens // block_size))\n",
    "\n",
    "print(f'Using block_size={block_size}, batch_size={batch_size}')\n"
   ],
   "id": "pick-hyperparams"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We use a stronger L4-oriented configuration with `gpt_fast` + bf16 to improve throughput while preserving quality."
   ],
   "id": "training-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptXUtIui79bC"
   },
   "outputs": [],
   "source": [
    "#@title Train (char-level GPT) on Colab L4\n",
    "!python -m yoctoGPT.train \\\n",
    "--mode char \\\n",
    "--data_dir data/char \\\n",
    "--ckpt_dir {CKPT_DIR} \\\n",
    "--model_type gpt_fast \\\n",
    "--device cuda \\\n",
    "--n_layer 8 --n_head 8 --n_embd 512 \\\n",
    "--block_size {block_size} --batch_size {batch_size} \\\n",
    "--dropout 0.1 --weight_decay 0.1 \\\n",
    "--tie_weights --label_smoothing 0.05 \\\n",
    "--amp --amp_dtype bf16 \\\n",
    "--auto_microbatch \\\n",
    "--eval_interval 500 --eval_iters 30 \\\n",
    "--cosine_lr --warmup_iters 200 \\\n",
    "--min_lr 1e-5 --lr 1.2e-4 \\\n",
    "--max_iters 4000 \\\n",
    "--ema --ema_decay 0.999\n"
   ],
   "id": "training-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling and Resuming\n",
    "\n",
    "After training, you can generate text or continue training from your saved checkpoints on Google Drive."
   ],
   "id": "sampling-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T3xSLf-79bD"
   },
   "outputs": [],
   "source": [
    "#@title Sample Continuation\n",
    "output_large_char = !python -m yoctoGPT.sampler \\\n",
    "--mode char \\\n",
    "--ckpt {CKPT_DIR}/best.pt \\\n",
    "--vocab_path data/char/vocab.json \\\n",
    "--prompt \"Philosophy is\" \\\n",
    "--max_new_tokens 300 \\\n",
    "--temperature 0.8 --top_k 50 --top_p 0.95\n",
    "\n",
    "generated_text_large_char = '\\n'.join(output_large_char)\n",
    "print(generated_text_large_char)\n"
   ],
   "id": "sampling-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "readability-intro-02"
   },
   "id": "readability-intro-02",
   "source": [
    "### Readability Assessment\n",
    "\n",
    "We evaluate generated text quality with the same readability metrics used in Notebook 01 for seamless comparison across experiments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "readability-code-02"
   },
   "id": "readability-code-02",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Analyze Generated Text Readability\n",
    "from textstat import textstat\n",
    "\n",
    "def readability_scores(text: str) -> dict:\n",
    "    return {\n",
    "        'Flesch Reading Ease': textstat.flesch_reading_ease(text),\n",
    "        'Flesch-Kincaid Grade': textstat.flesch_kincaid_grade(text),\n",
    "        'Dale-Chall Score': textstat.dale_chall_readability_score(text),\n",
    "        'Text Standard': textstat.text_standard(text, float_output=False),\n",
    "    }\n",
    "\n",
    "scores = readability_scores(generated_text_large_char)\n",
    "for metric, value in scores.items():\n",
    "    print(f'{metric:20}: {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56YUIamA79bE"
   },
   "outputs": [],
   "source": [
    "#@title Resume training from best.pt (lower LR + early stopping)\n",
    "best = CKPT_DIR / 'best.pt'\n",
    "if not best.exists():\n",
    "    print('No best.pt found; skipping resume cell.')\n",
    "else:\n",
    "    !python -m yoctoGPT.train \\\n",
    "--mode char \\\n",
    "--data_dir data/char \\\n",
    "--ckpt_dir {CKPT_DIR} \\\n",
    "--resume {best} \\\n",
    "--model_type gpt_fast \\\n",
    "--device cuda \\\n",
    "--n_layer 8 --n_head 8 --n_embd 512 \\\n",
    "--block_size {block_size} --batch_size {batch_size} \\\n",
    "--amp --amp_dtype bf16 \\\n",
    "--auto_microbatch \\\n",
    "--cosine_lr --warmup_iters 50 \\\n",
    "--lr 8e-5 --max_iters 1000 \\\n",
    "--eval_interval 250 --eval_iters 60 \\\n",
    "--early_stopping_patience 3 \\\n",
    "--early_stopping_min_delta 0.01\n"
   ],
   "id": "resume-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Architecture Trade-offs**: Try using `gpt_plus` (Accuracy-focused) instead of `gpt_fast`. Does it yield a lower validation loss for the same number of iterations?\n",
    "2. **Longer Context**: Increase the `block_size` to 1024 (if memory permits) and observe the coherence of the generated text.\n",
    "3. **Regularization**: Experiment with different `dropout` and `weight_decay` values to mitigate overfitting if the gap between train and val loss becomes too large."
   ],
   "id": "exercises"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-footer"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
