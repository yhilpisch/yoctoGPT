{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-logo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yoctoGPT \u2014 Minimal GPT from Scratch\n",
    "\n",
    "## _Scaling Up \u2014 Character-level Training_\n",
    "\n",
    "**&copy; Dr. Yves J. Hilpisch**<br>AI-Powered by OpenAI & Gemini."
   ],
   "id": "tae-header"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- **Goal**: Train a larger character-level GPT model on a combined corpus.\n",
    "- **Hardware**: Optimized for Google Colab T4 (~15GB VRAM).\n",
    "- **Persistence**: Mounts Google Drive for persistent checkpoint storage."
   ],
   "id": "how-to-use"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "1. **Setup**: Install dependencies, clone repo, and mount Google Drive.\n",
    "2. **Data**: Merge multiple text files into a large character-level corpus.\n",
    "3. **Training**: Train a larger model (8 layers, 512 embedding dim) with optimized settings.\n",
    "4. **Sampling & Resume**: Sample from checkpoints and learn how to resume training."
   ],
   "id": "roadmap"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-Zpkk_w79bA"
   },
   "outputs": [],
   "source": [
    "#@title Mount Google Drive for checkpoint storage (repo stays local)\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "CKPT_DIR = Path(\"/content/drive/MyDrive/yocto/checkpoints/colab_char\")\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Checkpoints dir:\", CKPT_DIR)"
   ],
   "id": "mount-drive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYn2jfRw79bB"
   },
   "outputs": [],
   "source": [
    "#@title Setup: Install Dependencies and Clone Repository\n",
    "!nvidia-smi || true\n",
    "!pip -q install tokenizers tqdm\n",
    "\n",
    "import os, pathlib, subprocess, textwrap\n",
    "\n",
    "repo_root = pathlib.Path(\"/content/yoctoGPT\")\n",
    "if repo_root.exists():\n",
    "    print(\"Repo exists, pulling latest...\")\n",
    "    subprocess.run([\"git\", \"pull\"], cwd=repo_root, check=False)\n",
    "else:\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"https://github.com/yhilpisch/yoctoGPT.git\",\n",
    "            str(repo_root),\n",
    "        ],\n",
    "        check=False,\n",
    "    )\n",
    "os.chdir(repo_root)\n",
    "\n",
    "if os.path.exists(\"requirements.txt\"):\n",
    "    !pip -q install -r requirements.txt || true\n",
    "\n",
    "data_dir = pathlib.Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "txts = list(data_dir.glob(\"*.txt\"))\n",
    "if not txts:\n",
    "    sample = textwrap.dedent('''\n",
    "    Philosophy is the study of general and fundamental questions,\n",
    "    such as those about existence, reason, knowledge, values, mind,\n",
    "    and language. It often poses questions rather than providing\n",
    "    answers, inviting us to think.\n",
    "    ''').strip()\n",
    "    (data_dir / \"philosophy.txt\").write_text(sample, encoding=\"utf-8\")\n",
    "    print(\"Created sample data/philosophy.txt\")\n",
    "else:\n",
    "    names = [p.name for p in txts][:5]\n",
    "    print(f\"Found {len(txts)} text files in data/: {names}\")"
   ],
   "id": "setup-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We merge all available text files in the `data/` directory to create a larger, more diverse training set for our character-level model."
   ],
   "id": "data-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "425J9Hmm79bB"
   },
   "outputs": [],
   "source": [
    "#@title Merge Texts and Prepare Char-level Dataset\n",
    "data_dir = pathlib.Path(\"data\")\n",
    "texts = []\n",
    "for p in sorted(data_dir.glob(\"*.txt\")):\n",
    "    print(\"Including:\", p)\n",
    "    texts.append(p.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "merged_path = data_dir / \"all_texts_char.txt\"\n",
    "merged_path.write_text(\"\\n\\n\".join(texts), encoding=\"utf-8\")\n",
    "print(\"Wrote merged corpus to\", merged_path)\n",
    "\n",
    "!python -m scripts.prepare_char_data \\\n",
    "  --text_path data/all_texts_char.txt \\\n",
    "  --out_dir data/char"
   ],
   "id": "data-code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpPpJAYs79bC"
   },
   "outputs": [],
   "source": [
    "#@title Pick safe block_size/batch_size\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "train_path = Path(\"data/char/train.bin\")\n",
    "val_path = Path(\"data/char/val.bin\")\n",
    "train_tokens = int(np.fromfile(train_path, dtype=np.int32).shape[0])\n",
    "val_tokens = int(np.fromfile(val_path, dtype=np.int32).shape[0])\n",
    "min_tokens = min(train_tokens, val_tokens)\n",
    "\n",
    "block_candidates = [512, 384, 256, 192, 128, 96, 64, 48, 32, 24, 16]\n",
    "block_size = next(\n",
    "    (b for b in block_candidates if min_tokens > b + 2),\n",
    "    max(8, min_tokens - 2),\n",
    ")\n",
    "\n",
    "target_tokens = min(16000, max(512, min_tokens))\n",
    "batch_size = max(1, min(24, target_tokens // block_size))\n",
    "\n",
    "print(f\"Using block_size={block_size}, batch_size={batch_size}\")"
   ],
   "id": "pick-hyperparams"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We use a more powerful configuration with 8 layers and 512 dimensions. We also enable features like EMA (Exponential Moving Average) and weight tying for better performance."
   ],
   "id": "training-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptXUtIui79bC"
   },
   "outputs": [],
   "source": [
    "#@title Train (char-level GPT) on Colab T4\n",
    "!python -m yoctoGPT.train \\\n",
    "  --mode char \\\n",
    "  --data_dir data/char \\\n",
    "  --ckpt_dir {CKPT_DIR} \\\n",
    "  --model_type gpt_fast \\\n",
    "  --device cuda \\\n",
    "  --n_layer 8 --n_head 8 --n_embd 512 \\\n",
    "  --block_size {block_size} --batch_size {batch_size} \\\n",
    "  --dropout 0.1 --weight_decay 0.1 \\\n",
    "  --tie_weights --label_smoothing 0.05 \\\n",
    "  --eval_interval 800 --eval_iters 100 \\\n",
    "  --cosine_lr --warmup_iters 400 \\\n",
    "  --min_lr 1e-5 --lr 2e-4 \\\n",
    "  --max_iters 6000 \\\n",
    "  --ema --ema_decay 0.999"
   ],
   "id": "training-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling and Resuming\n",
    "\n",
    "After training, you can generate text or continue training from your saved checkpoints on Google Drive."
   ],
   "id": "sampling-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T3xSLf-79bD"
   },
   "outputs": [],
   "source": [
    "#@title Sample Continuation\n",
    "!python -m yoctoGPT.sampler \\\n",
    "  --mode char \\\n",
    "  --ckpt {CKPT_DIR}/latest.pt \\\n",
    "  --vocab_path data/char/vocab.json \\\n",
    "  --prompt \"Philosophy is\" \\\n",
    "  --max_new_tokens 300 \\\n",
    "  --temperature 0.8 --top_k 50 --top_p 0.95"
   ],
   "id": "sampling-code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56YUIamA79bE"
   },
   "outputs": [],
   "source": [
    "#@title Resume training from latest.pt\n",
    "latest = CKPT_DIR / \"latest.pt\"\n",
    "if not latest.exists():\n",
    "    print(\"No latest.pt found; skipping resume cell.\")\n",
    "else:\n",
    "    !python -m yoctoGPT.train \\\n",
    "      --mode char \\\n",
    "      --data_dir data/char \\\n",
    "      --ckpt_dir {CKPT_DIR} \\\n",
    "      --resume {latest} \\\n",
    "      --model_type gpt_fast \\\n",
    "      --device cuda \\\n",
    "      --n_layer 8 --n_head 8 --n_embd 512 \\\n",
    "      --block_size {block_size} --batch_size {batch_size} \\\n",
    "      --max_iters 1000"
   ],
   "id": "resume-code"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. **Architecture Trade-offs**: Try using `gpt_plus` (Accuracy-focused) instead of `gpt_fast`. Does it yield a lower validation loss for the same number of iterations?\n",
    "2. **Longer Context**: Increase the `block_size` to 1024 (if memory permits) and observe the coherence of the generated text.\n",
    "3. **Regularization**: Experiment with different `dropout` and `weight_decay` values to mitigate overfitting if the gap between train and val loss becomes too large."
   ],
   "id": "exercises"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=\"35%\" align=\"right\">\n"
   ],
   "id": "tae-footer"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
